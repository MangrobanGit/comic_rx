{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comics Rx\n",
    "## [A comic book recommendation system](https://github.com/MangrobanGit/comics_rx)\n",
    "<img src=\"https://images.unsplash.com/photo-1514329926535-7f6dbfbfb114?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2850&q=80\" width=\"400\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. ALS with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2  # 1 would be where you need to specify the files\n",
    "#%aimport data_fcns\n",
    "\n",
    "import pandas as pd # dataframes\n",
    "import os\n",
    "import gspread_pandas\n",
    "from gspread_pandas import Spread, Client # gsheets interaction\n",
    "\n",
    "# Data storage\n",
    "from sqlalchemy import create_engine # SQL helper\n",
    "import psycopg2 as psql #PostgreSQL DBs\n",
    "\n",
    "# import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.sql.types import (StructType, StructField, IntegerType\n",
    "#                                ,FloatType, LongType, StringType)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, lit, isnan, when, count\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Custom\n",
    "import data_fcns as dfc\n",
    "import keys  # Custom keys lib\n",
    "import comic_recs as cr\n",
    "\n",
    "# Data storage\n",
    "from sqlalchemy import create_engine # SQL helper\n",
    "import psycopg2 as psql #PostgreSQL DBs\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate SparkSession object\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "In a previous NB we have already scrubbed the data to get into proper format for ALS. We saved to JSON to make it easy to re-ingest for ALS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+\n",
      "|account_id|bought|comic_id|\n",
      "+----------+------+--------+\n",
      "|      2247|     1|     995|\n",
      "|       487|     1|    1102|\n",
      "|        29|     1|    2680|\n",
      "|      1260|     1|    4870|\n",
      "|       172|     1|    6023|\n",
      "|      2493|     1|      66|\n",
      "|        52|     1|     116|\n",
      "|        32|     1|     755|\n",
      "|      1149|     1|     971|\n",
      "|      1489|     1|    3503|\n",
      "+----------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comics_sold = spark.read.json('raw_data/als_input_filtered.json')\n",
    "comics_sold.persist()\n",
    "\n",
    "comics_sold.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Model\n",
    "\n",
    "Let's start with  train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "(train, test) = comics_sold.randomSplit([.8, .2], seed=41916)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure shapes make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44197 3\n"
     ]
    }
   ],
   "source": [
    "print(train.count(), len(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11145 3\n"
     ]
    }
   ],
   "source": [
    "print(test.count(), len(test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3rd Run @ ALS with filtered dataset\n",
    "\n",
    "- Minimum number of titles bought by account = 5\n",
    "- Maximum number of titles bought by account = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started on Wed Jun 26 21:11:41 2019.\n"
     ]
    }
   ],
   "source": [
    "now = time.ctime(int(time.time()))\n",
    "print(\"Started on {}.\".format(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance and fit model\n",
    "als = ALS(maxIter=20,\n",
    "          rank=10,\n",
    "          userCol='account_id',\n",
    "          itemCol='comic_id',\n",
    "          ratingCol='bought',\n",
    "          alpha=.01,\n",
    "          implicitPrefs=True,\n",
    "          seed=41916)\n",
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed on Wed Jun 26 21:11:56 2019.\n"
     ]
    }
   ],
   "source": [
    "now = time.ctime(int(time.time()))\n",
    "print(\"Completed on {}.\".format(now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[account_id: bigint, bought: bigint, comic_id: bigint, prediction: float]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions on TEST\n",
    "predictions = model.transform(test)\n",
    "predictions.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+-------------+\n",
      "|account_id|bought|comic_id|   prediction|\n",
      "+----------+------+--------+-------------+\n",
      "|       336|     1|     471|   0.08173462|\n",
      "|      1110|     1|     471|   0.07119954|\n",
      "|      1842|     1|     471|  0.046360236|\n",
      "|       691|     1|    1088|  0.012279962|\n",
      "|       125|     1|    1342|  0.009202151|\n",
      "|       296|     1|    1959|  0.009963108|\n",
      "|       731|     1|    1959|  0.094873644|\n",
      "|      2223|     1|    1959|  0.056293443|\n",
      "|        49|     1|    1959|   0.00999463|\n",
      "|       708|     1|    1959|-0.0026608421|\n",
      "+----------+------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BinaryClassificationEvaluator` only likes doubles for `rawPredictionCol`, so cast it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------------------+\n",
      "|account_id|bought|comic_id|          prediction|\n",
      "+----------+------+--------+--------------------+\n",
      "|       336|     1|     471| 0.08173462003469467|\n",
      "|      1110|     1|     471| 0.07119953632354736|\n",
      "|      1842|     1|     471|0.046360235661268234|\n",
      "|       691|     1|    1088|0.012279962189495564|\n",
      "|       125|     1|    1342|0.009202150627970695|\n",
      "|       296|     1|    1959|0.009963108226656914|\n",
      "|       731|     1|    1959| 0.09487364441156387|\n",
      "|      2223|     1|    1959| 0.05629344284534454|\n",
      "|        49|     1|    1959| 0.00999462977051735|\n",
      "|       708|     1|    1959|-0.00266084214672...|\n",
      "+----------+------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Evaluation\n",
    "\n",
    "Based on our first swing of the bat:\n",
    "- `maxIter` = 20\n",
    "- `rank` = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC'\n",
    "                                          ,labelCol='bought'\n",
    "                                          ,rawPredictionCol='prediction'\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under the Curve = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Area Under the Curve = \" + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **REALLY?** \n",
    "Smells weird. Especially when we look at the actual `prediction` values. How can it be `1` when even just the sample I took is showing basically `0`'s for all predictions???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "pred_df = predictions.select('*').toPandas()\n",
    "\n",
    "# Check nulls\n",
    "pred_num_nulls = pred_df['prediction'].isna().sum()\n",
    "\n",
    "# Num rows\n",
    "preds_attempted = pred_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 374 nulls out of 11145.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} nulls out of {}.\".format(pred_num_nulls, preds_attempted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So 0.03% are nulls.\n"
     ]
    }
   ],
   "source": [
    "print(\"So {:.2f}% are nulls.\".format(pred_num_nulls/preds_attempted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, they're such a tiny portion of the test set, for now let's remove the nulls for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11145"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to spark dataframe\n",
    "predictions = spark.createDataFrame(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+----------+\n",
      "|account_id|bought|comic_id|prediction|\n",
      "+----------+------+--------+----------+\n",
      "|         0|     0|       0|       374|\n",
      "+----------+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select([count(when(isnan(c), c)).alias(c) for c in predictions.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_no_na = predictions.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[account_id: bigint, bought: bigint, comic_id: bigint, prediction: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_no_na.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10771"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_no_na.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the AUC on the test data\n",
    "auc2 = evaluator.evaluate(pred_no_na)\n",
    "\n",
    "print(\"AUC = \" + str(auc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still the same: AUC 1.0, but that makes sense, relative to what we did (just remove some rows). Also, how did we get an AUC before if there were NAN's???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that gives us a baseline. Let's switch gears and do a little grid search action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do that, some testing shows that I can't get the BinaryEvaluator to work because I need to manually cast the `predictionCol` to double, but can't do that while encapsulated inside of the CrossValidator class. So will do RMSE _for now_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "eval_reg = RegressionEvaluator(metricName=\"rmse\", labelCol=\"bought\",\n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parameter grid\n",
    "params = (ParamGridBuilder()\n",
    "          .addGrid(als.regParam, [1, 0.01, 0.001, 0.1])\n",
    "          .addGrid(als.maxIter, [5, 10, 20])\n",
    "          .addGrid(als.rank, [4, 10, 50])\n",
    "          .addGrid(als.alpha, [.01, 0.1, 1, 10, 40])).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv = TrainValidationSplit(estimator=als\n",
    "                          , evaluator=eval_reg\n",
    "                          , estimatorParamMaps=params\n",
    "                          , trainRatio=0.8)\n",
    "#cv = CrossValidator(estimator=als_implicit, estimatorParamMaps=paramGrid, evaluator=RegressionEvaluator())\n",
    "model_implicit = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross validate for best hyperparameters\n",
    "cv = CrossValidator(estimator=als\n",
    "                    ,estimatorParamMaps=params\n",
    "                    ,evaluator=eval_reg\n",
    "                    ,parallelism=4\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.ctime(int(time.time()))\n",
    "print(\"Completed on {}.\".format(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and store model\n",
    "best_model = cv.fit(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_model = best_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.ctime(int(time.time()))\n",
    "print(\"Completed on {}.\".format(now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV: Using RMSE. Bleh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top N recommendations for Single User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a reference list of `account_id`'s, for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_test = 5\n",
    "\n",
    "users = (tot_sold_ids_only.select(als.getUserCol())\n",
    "                          .sample(False\n",
    "                                  ,n_to_test/tot_sold_ids_only.count()\n",
    "                                  ,41916)\n",
    "        )\n",
    "users.persist()\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed and wrote the functionality out to a function in `comic_recs.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing function!\n",
    "\n",
    "- Pass the function to a pandas dataframe. \n",
    "- Function will ask for an account_id.\n",
    "- Will return top n, n defined in parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_recs_for_user(spark=spark, model=model, topn=5)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_recs_for_user(spark=spark, model=model, topn=5)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_recs_for_user(spark=spark, model=model, topn=5)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- OK this seems more realistic(?) Only three tests, but it seems more realistic that two users would completely different recommendations, than other results I've seen to date, where there would be overlap of 2 or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
