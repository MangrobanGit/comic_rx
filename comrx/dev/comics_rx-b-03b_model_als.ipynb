{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comics Rx\n",
    "## [A comic book recommendation system](https://github.com/MangrobanGit/comics_rx)\n",
    "<img src=\"https://images.unsplash.com/photo-1514329926535-7f6dbfbfb114?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2850&q=80\" width=\"400\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced Data: Grid Search + Cross-Validation\n",
    "\n",
    "This time, as explored in the EDA NB, let's consider removing customers who we feel have too few or too many purchases to influence the model in the intended way.\n",
    "\n",
    "Examples:\n",
    "- Too few - Customers who have only bought 1 comic (series).\n",
    "- Too many - Customers with > 1000 series (for example, think all eBay customers are rolled into one account number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "# %autoreload 1 #would be where you need to specify the files\n",
    "# %aimport comic_recs\n",
    "\n",
    "import pandas as pd # dataframes\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Data storage\n",
    "from sqlalchemy import create_engine # SQL helper\n",
    "import psycopg2 as psql #PostgreSQL DBs\n",
    "\n",
    "# import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.sql.types import (StructType, StructField, IntegerType\n",
    "#                                ,FloatType, LongType, StringType)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, lit, isnan, when, count\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import (CrossValidator, ParamGridBuilder, \n",
    "                               TrainValidationSplit)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom\n",
    "import data_fcns as dfc\n",
    "import keys  # Custom keys lib\n",
    "import comic_recs as cr\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# Laptop config\n",
    "# conf = (conf.setMaster('local[*]')\n",
    "# #         .set('spark.executor.memory', '1G') #https://stackoverflow.com/questions/48523629/spark-pyspark-an-error-occurred-while-trying-to-connect-to-the-java-server-127\n",
    "#         .set('spark.driver.memory', '7G')\n",
    "#         .set('spark.driver.maxResultSize', '2G'))\n",
    "# #         .set('spark.executor.memory', '1G')\n",
    "# #         .set('spark.driver.memory', '10G')\n",
    "# #         .set('spark.driver.maxResultSize', '5G'))\n",
    "\n",
    "# AWS instance config\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.driver.memory', '25G')\n",
    "        .set('spark.driver.maxResultSize', '4G'))\n",
    "#         .set('spark.executor.memory', '1G') #https://stackoverflow.com/questions/48523629/spark-pyspark-an-error-occurred-while-trying-to-connect-to-the-java-server-127\n",
    "#         .set('spark.driver.memory', '10G')\n",
    "#         .set('spark.driver.maxResultSize', '5G'))\n",
    "\n",
    "sc = pyspark.SparkContext().getOrCreate(conf=conf)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "sc.setCheckpointDir('./checkpoints')\n",
    "\n",
    "# spark.sparkContext.setCheckpointDir(\"hdfs://datalake/check_point_directory/als\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "We've previously set aside the dataset into a `json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have previously created a version of the transactions table \n",
    "# and filtered it down.\n",
    "sold = sql_context.read.json('raw_data/als_input_filtered_190915.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[account_id: bigint, bought: bigint, comic_id: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist the data\n",
    "sold.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846090"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sold.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Model\n",
    "\n",
    "Let's start with  train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "(train, test) = sold.randomSplit([.75, .25], seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634262"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure shapes make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634262 3\n"
     ]
    }
   ],
   "source": [
    "print(train.count(), len(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211828 3\n"
     ]
    }
   ],
   "source": [
    "print(test.count(), len(test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper-param config\n",
    "# max_iters = [20, 25, 30]\n",
    "# ranks = [5, 10, 20, 25]\n",
    "# reg_params = [.05, .10, .15]\n",
    "# alphas = [5, 25, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper-param config\n",
    "# max_iters = [30, 40]\n",
    "# ranks = [25, 30]\n",
    "# reg_params = [.15, .25]\n",
    "# alphas = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper-param config\n",
    "# max_iters = [30]\n",
    "# ranks = [5, 10]\n",
    "# reg_params = [.25, .30]\n",
    "# alphas = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper-param config\n",
    "# max_iters = [40]\n",
    "# ranks = [30]\n",
    "# reg_params = [.25]\n",
    "# alphas = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "max_iters = [20, 30, 40]\n",
    "ranks = [5, 10, 20, 30, 40]\n",
    "reg_params = [.05, .10, .25, .50]\n",
    "alphas = [5, 25, 40, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate list to hold candidate models\n",
    "model_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop will automatically create and store ALS models\n",
    "def create_als_models_grid(userCol, itemCol, ratingCol, ):\n",
    "    for r in ranks:\n",
    "        for mi in max_iters:\n",
    "            for rp in reg_params:\n",
    "                for a in alphas:\n",
    "                    model_list.append(ALS(userCol= \"account_id\"\n",
    "                                          ,itemCol= \"comic_id\"\n",
    "                                          ,ratingCol= \"bought\"\n",
    "                                          ,rank = r, maxIter = mi, regParam = rp\n",
    "                                          ,alpha = a\n",
    "                                          ,coldStartStrategy=\"drop\"\n",
    "                                          ,nonnegative = True\n",
    "                                          ,implicitPrefs = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop will automatically create and store ALS models\n",
    "for r in ranks:\n",
    "    for mi in max_iters:\n",
    "        for rp in reg_params:\n",
    "            for a in alphas:\n",
    "                model_list.append(ALS(userCol= \"account_id\"\n",
    "                                      ,itemCol= \"comic_id\"\n",
    "                                      ,ratingCol= \"bought\"\n",
    "                                      ,rank = r, maxIter = mi, regParam = rp\n",
    "                                      ,alpha = a\n",
    "                                      ,coldStartStrategy=\"drop\"\n",
    "                                      ,nonnegative = True\n",
    "                                      ,implicitPrefs = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max_iters)*len(ranks)*len(reg_params)*len(alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building 5 folds within the training set.\n",
    "train1, train2, train3, train4, train5 = (train.randomSplit(\n",
    "                                         [0.2, 0.2, 0.2, 0.2, 0.2], seed = 1234))\n",
    "fold1 = train2.union(train3).union(train4).union(train5)\n",
    "fold2 = train3.union(train4).union(train5).union(train1)\n",
    "fold3 = train4.union(train5).union(train1).union(train2)\n",
    "fold4 = train5.union(train1).union(train2).union(train3)\n",
    "fold5 = train1.union(train2).union(train3).union(train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldlist = [(fold1, train1), (fold2, train2), (fold3, train3)\n",
    "            , (fold4, train4), (fold5, train5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was initially created for the Million Songs Echo Nest Taste Profile dataset which had 3 columns: userId, songId,\n",
    "# and num_plays. The column num_plays was used as implicit ratings with the ALS algorithm.\n",
    "\n",
    "def ROEM(predictions):\n",
    "    #Creates predictions table that can be queried\n",
    "    predictions.createOrReplaceTempView(\"predictions\") \n",
    "\n",
    "    #Sum of total number of plays of all songs\n",
    "    denominator = predictions.groupBy().sum(\"bought\").collect()[0][0]\n",
    "\n",
    "    #Calculating rankings of songs predictions by user\n",
    "    #spark.sql(\"SELECT account_id, bought, PERCENT_RANK() OVER (PARTITION BY account_id ORDER BY prediction DESC) AS rank FROM predictions\").createOrReplaceTempView(\"rankings\")\n",
    "    sql_context.sql(\"SELECT account_id, bought, PERCENT_RANK() OVER (PARTITION BY account_id ORDER BY prediction DESC) AS rank FROM predictions\").createOrReplaceTempView(\"rankings\")\n",
    "\n",
    "    #Multiplies the rank of each song by the number of plays for each user\n",
    "    #and adds the products together\n",
    "    #  numerator = spark.sql('SELECT SUM(bought * rank) FROM rankings').collect()[0][0]\n",
    "    numerator = sql_context.sql('SELECT SUM(bought * rank) FROM rankings').collect()[0][0]\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to fill with ROEMs from each model\n",
    "ROEMS = []\n",
    "\n",
    "n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through all models and all folds\n",
    "for model in model_list:\n",
    "#     for ft_pair in foldlist:\n",
    "\n",
    "#         # Fits model to fold within training data\n",
    "#         fitted_model = model.fit(ft_pair[0])\n",
    "\n",
    "#         # Generates predictions using fitted_model on respective CV test data\n",
    "#         predictions = fitted_model.transform(ft_pair[1])\n",
    "\n",
    "#         # Generates and prints a ROEM metric CV test data\n",
    "#         r = ROEM(predictions)\n",
    "#         print (\"ROEM: \", r)\n",
    "\n",
    "    # Fits model to all of training data and generates preds for test data\n",
    "    v_fitted_model = model.fit(train)\n",
    "    v_predictions = v_fitted_model.transform(test)\n",
    "    v_ROEM = ROEM(v_predictions)\n",
    "\n",
    "    # Adds validation ROEM to ROEM list\n",
    "    ROEMS.append(v_ROEM)\n",
    "    print (\"Validation ROEM #{}: {}\".format(n, v_ROEM))\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ROEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the smallest ROEM\n",
    "idx = np.argmin(ROEMS)\n",
    "print(\"Index of smallest ROEM:\", idx)\n",
    "\n",
    "# Find ith element of ROEMS\n",
    "print(\"Smallest ROEM: \", ROEMS[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best_model\n",
    "best_model = model_list[idx]\n",
    "\n",
    "print('alpha: {}'.format(best_model.getAlpha()))\n",
    "print('reg param: {}'.format(best_model.getRegParam()))\n",
    "print('max iter: {}'.format(best_model.getMaxIter()))\n",
    "print('rank: {}'.format(best_model.getRank()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('models/best_model_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('support_data/best_model_20190916a.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_model, f)\n",
    "    \n",
    "# # Example - load pickle\n",
    "# # pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# # pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in = open('support_data/params_errs_20190912d.pkl', 'rb')\n",
    "# params_errs_4 = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft_pair in foldlist:\n",
    "\n",
    "    # Fits model to fold within training data\n",
    "    fitted_model = best_model.fit(ft_pair[0])\n",
    "\n",
    "    # Generates predictions using fitted_model on respective CV test data\n",
    "    predictions = fitted_model.transform(ft_pair[1])\n",
    "\n",
    "    # Generates and prints a ROEM metric CV test data\n",
    "    r = ROEM(predictions)\n",
    "    errors.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CV errors range: %0.2f (+/- %0.2f)\" % (np.mean(errors), np.std(errors) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty stable, which helps alleviate some over-fitting concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Candidate Model\n",
    "\n",
    "Test vs our holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitted = best_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test\n",
    "test_preds = model_fitted.transform(test)\n",
    "\n",
    "# Evaluate test\n",
    "test_roem = ROEM(test_preds)\n",
    "test_roem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird that test error is lower than train...but not too far off. Let's run with it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the item factors for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = model_fitted.itemFactors.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.to_pickle(\"support_data/item_factors_20190916.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = item_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_items = pd.read_pickle('support_data/item_factors_20190916.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_errors(folds, als, evaluator):\n",
    "    \"\"\"\n",
    "    Given dictionary of spark DF folds and an ALS object\n",
    "    returns list of errors\n",
    "    Parameters\n",
    "    ----------\n",
    "    folds = list of spark dataframes\n",
    "    als = ALS instance\n",
    "    evaluator = spark Evaluator instance, usually regression\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of each test fold's prediction error metric\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for i in range(len(folds)):\n",
    "\n",
    "        # Partition out train and test\n",
    "        test_fold_df = folds[i]\n",
    "\n",
    "        train_folds = list(set(folds) - set([test_fold_df]))\n",
    "        train_fold_df = reduce(DataFrame.unionAll, train_folds)\n",
    "\n",
    "        # fit on train\n",
    "        model = als.fit(train_fold_df)\n",
    "\n",
    "        # get predictions on test\n",
    "        preds = model.transform(test_fold_df)\n",
    "\n",
    "        # Evaluate test\n",
    "        errors.append(evaluator.evaluate(preds))\n",
    "\n",
    "        # done\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "eval_reg = RegressionEvaluator(metricName=\"rmse\"\n",
    "                               , labelCol=\"bought\"\n",
    "                               , predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further subset into test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "(gs_train, gs_val) = train.randomSplit([(1-(1/3)), (1/3)], seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_train.count(), len(gs_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_val.count(), len(gs_val.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = [5, 10, 20]\n",
    "ranks = [5, 10, 20]\n",
    "reg_params = [0.01, 0.1, 1]\n",
    "alphas = [5, 25, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the descriptive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_errs_rd_1 = params_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support_data/params_errs_20190912a.pkl', 'wb') as f:\n",
    "    pickle.dump(param_errs_rd_1, f)\n",
    "    \n",
    "# Example - load pickle\n",
    "# pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('support_data/params_errs_20190912a.pkl', 'rb')\n",
    "params_errs_1 = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. Let's put `params_errs_1` into a dataframe and find the model with the lowest error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cols = ['max_iters', 'reg', 'rank', 'alpha', 'rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(params_errs_1, columns=gs_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_err = gs_df.rmse.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = gs_df.loc[gs_df['rmse']==min_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_iter = min_df['max_iters'].iloc[0]\n",
    "best_reg = min_df['reg'].iloc[0]\n",
    "best_rank = min_df['rank'].iloc[0]\n",
    "best_alpha = min_df['alpha'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some visual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rank_match = (gs_df['rank']==best_rank)\n",
    "gs_reg_match = (gs_df['reg']==best_reg)\n",
    "gs_iter_match = (gs_df['max_iters']==best_max_iter)\n",
    "gs_alpha_match = (gs_df['alpha']==best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank = gs_df.loc[(gs_reg_match & gs_iter_match & gs_alpha_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha = gs_df.loc[(gs_reg_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg = gs_df.loc[(gs_alpha_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter = gs_df.loc[(gs_alpha_match & gs_reg_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So quick inspection on these, lets:\n",
    "- Keep `rank` = 5  \n",
    "- Stick with regularization parameter at `0.01`.  \n",
    "- Max iterations it seems will plateau, but let's investigate going up to maybe 30 or so.\n",
    "- Looking like going higher on alpha might lower RMSE more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's go through another grid of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = [20, 30, 40]\n",
    "ranks = [5]\n",
    "reg_params = [0.01]\n",
    "alphas = [40, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs_2 = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support_data/params_errs_20190912b.pkl', 'wb') as f:\n",
    "    pickle.dump(params_errs_2, f)\n",
    "    \n",
    "# Example - load pickle\n",
    "# pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('support_data/params_errs_20190912b.pkl', 'rb')\n",
    "params_errs_2 = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing iterations doesn't seem t materially improve RMSE, but increasing alpha does. So let's try a large range of alphas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = [20]\n",
    "alphas = [50, 100, 500, 1000, 1500, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs_3 = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support_data/params_errs_20190912c.pkl', 'wb') as f:\n",
    "    pickle.dump(params_errs_3, f)\n",
    "    \n",
    "# Example - load pickle\n",
    "# pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('support_data/params_errs_20190912c.pkl', 'rb')\n",
    "params_errs_3 = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears that `alpha` = 1000 is where min RMSE is so far. For due diligence let's look at fewer latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [2,3,4,5,10,20,30,50]\n",
    "alphas = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs_4 = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support_data/params_errs_20190912d.pkl', 'wb') as f:\n",
    "    pickle.dump(params_errs_4, f)\n",
    "    \n",
    "# Example - load pickle\n",
    "# pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('support_data/params_errs_20190912d.pkl', 'rb')\n",
    "params_errs_4 = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 3 is the number of latent factors that minimizes RMSE. \n",
    "\n",
    "Let's put together a final run grid that we can use to compare different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = [10, 20, 30]\n",
    "ranks = [3,5,10]\n",
    "reg_params = [0.01, 0.1,0.5]\n",
    "alphas = [40, 50, 100, 1000, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs_5 = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support_data/params_errs_20190912e.pkl', 'wb') as f:\n",
    "    pickle.dump(params_errs_5, f)\n",
    "    \n",
    "# Example - load pickle\n",
    "# pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('support_data/params_errs_20190912e.pkl', 'rb')\n",
    "params_errs_5 = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = [20]\n",
    "ranks = [3]\n",
    "reg_params = [0.01, 0.1,0.5,0.7,1]\n",
    "alphas = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs_x = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ALS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cols = ['max_iters', 'reg', 'rank', 'alpha', 'rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(params_errs_5, columns=gs_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_err = gs_df.rmse.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = gs_df.loc[gs_df['rmse']==min_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_iter = min_df['max_iters'].iloc[0]\n",
    "best_reg = min_df['reg'].iloc[0]\n",
    "best_rank = min_df['rank'].iloc[0]\n",
    "best_alpha = min_df['alpha'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some visual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rank_match = (gs_df['rank']==best_rank)\n",
    "gs_reg_match = (gs_df['reg']==best_reg)\n",
    "gs_iter_match = (gs_df['max_iters']==best_max_iter)\n",
    "gs_alpha_match = (gs_df['alpha']==best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank = gs_df.loc[(gs_reg_match & gs_iter_match & gs_alpha_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha = gs_df.loc[(gs_reg_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg = gs_df.loc[(gs_alpha_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter = gs_df.loc[(gs_alpha_match & gs_reg_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df.sort_values(['rmse']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So quick inspection on these, lets:\n",
    "- keep `rank` = 5\n",
    "- When compared to all the other combos, the differences in `alpha`s seem to not really move the needle > 500. so let's just call it `1000`\n",
    "- Keep `maxIter` at `20`; experience to date with my assets seems to show 20 is max capability before technical difficulties arise.\n",
    "- Similar with `alpha`, the marginal change in error due to changing `reg` is really small. So let's just assume the default `.01`.\n",
    "\n",
    "So, that means we are done selecting! We may really be pushing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing, let chart change in RMSE over change in alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df = gs_vary_alpha.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df['params_desc'] = (\n",
    "                                '\\u03B1=' + alpha_graph_df['alpha'].map(str) \n",
    "                                )\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot RMSE\n",
    "sns.set_color_codes(\"pastel\")\n",
    "\n",
    "values = alpha_graph_df['params_desc'].tolist()\n",
    "\n",
    "clrs = ['salmon' if (y == '\\u03B1=1000') else 'steelblue' for y in values ]\n",
    "\n",
    "s = sns.barplot(x=\"rmse\", y=\"params_desc\", data=alpha_graph_df,\n",
    "                label=\"RMSE\",\n",
    "                palette=clrs)\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "\n",
    "xlabel = \"Max Iterations: \" + str(best_max_iter) + \" | Latent Factors: \" + str(best_rank)\n",
    "# xlabel\n",
    "\n",
    "\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"\",\n",
    "       xlabel=xlabel)\n",
    "ax.set_title(\"Change in Error over Alpha\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "fig = s.get_figure()\n",
    "# fig.savefig('support_data/alphas_20190905.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing, let's chart change in RMSE over change in alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df = gs_vary_alpha.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df['params_desc'] = (\n",
    "                                '\\u03B1=' + alpha_graph_df['alpha'].map(str) \n",
    "                                )\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot RMSE\n",
    "sns.set_color_codes(\"pastel\")\n",
    "\n",
    "values = alpha_graph_df['params_desc'].tolist()\n",
    "\n",
    "clrs = ['salmon' if (y == '\\u03B1=1000') else 'steelblue' for y in values ]\n",
    "\n",
    "s = sns.barplot(x=\"rmse\", y=\"params_desc\", data=alpha_graph_df,\n",
    "                label=\"RMSE\",\n",
    "                palette=clrs)\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "\n",
    "xlabel = \"Max Iterations: \" + str(best_max_iter) + \" | Latent Factors: \" + str(best_rank)\n",
    "# xlabel\n",
    "\n",
    "\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"\",\n",
    "       xlabel=xlabel)\n",
    "ax.set_title(\"Change in Error over Alpha\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "fig = s.get_figure()\n",
    "fig.savefig('support_data/alphas_20190905.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note those that iterations = 35 is the max of our grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = [20, 25, 40]\n",
    "ranks = [4,5]\n",
    "reg_params = [0.01, 0.1]\n",
    "alphas = [40, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_errs_rd_2 = params_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support_data/params_errs_20190907b.pkl', 'wb') as f:\n",
    "    pickle.dump(param_errs_rd_2, f)\n",
    "    \n",
    "# Example - load picklea\n",
    "# pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('support_data/params_errs_20190907b.pkl', 'rb')\n",
    "params_errs_2 = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cols = ['max_iters', 'reg', 'rank', 'alpha', 'rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(params_errs_2, columns=gs_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_err = gs_df.rmse.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = gs_df.loc[gs_df['rmse']==min_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_iter = min_df['max_iters'].iloc[0]\n",
    "best_reg = min_df['reg'].iloc[0]\n",
    "best_rank = min_df['rank'].iloc[0]\n",
    "best_alpha = min_df['alpha'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some visual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rank_match = (gs_df['rank']==best_rank)\n",
    "gs_reg_match = (gs_df['reg']==best_reg)\n",
    "gs_iter_match = (gs_df['max_iters']==best_max_iter)\n",
    "gs_alpha_match = (gs_df['alpha']==best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank = gs_df.loc[(gs_reg_match & gs_iter_match & gs_alpha_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha = gs_df.loc[(gs_reg_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg = gs_df.loc[(gs_alpha_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter = gs_df.loc[(gs_alpha_match & gs_reg_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So quick inspection on these, lets:\n",
    "- keep `rank` = 5\n",
    "- When compared to all the other combos, the differences in `alpha`s seem to not really move the needle > 500. so let's just call it `1000`\n",
    "- Keep `maxIter` at `20`; experience to date with my assets seems to show 20 is max capability before technical difficulties arise.\n",
    "- Similar with `alpha`, the marginal change in error due to changing `reg` is really small. So let's just assume the default `.01`.\n",
    "\n",
    "So, that means we are done selecting! We may really be pushing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing, let chart change in RMSE over change in alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df = gs_vary_alpha.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df['params_desc'] = (\n",
    "                                '\\u03B1=' + alpha_graph_df['alpha'].map(str) \n",
    "                                )\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot RMSE\n",
    "sns.set_color_codes(\"pastel\")\n",
    "\n",
    "values = alpha_graph_df['params_desc'].tolist()\n",
    "\n",
    "clrs = ['salmon' if (y == '\\u03B1=1000') else 'steelblue' for y in values ]\n",
    "\n",
    "s = sns.barplot(x=\"rmse\", y=\"params_desc\", data=alpha_graph_df,\n",
    "                label=\"RMSE\",\n",
    "                palette=clrs)\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "\n",
    "xlabel = \"Max Iterations: \" + str(best_max_iter) + \" | Latent Factors: \" + str(best_rank)\n",
    "# xlabel\n",
    "\n",
    "\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"\",\n",
    "       xlabel=xlabel)\n",
    "ax.set_title(\"Change in Error over Alpha\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "fig = s.get_figure()\n",
    "#fig.savefig('support_data/alphas_20190905.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = [20]\n",
    "ranks = [3,4]\n",
    "reg_params = [0.01]\n",
    "alphas = [100, 500, 1000, 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = [30, 50, 75, 100]\n",
    "ranks = [5]\n",
    "reg_params = [1, 5, 10]\n",
    "alphas = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK**. Let's call it good. \n",
    "\n",
    "## Results \n",
    "Looks like the best parameters we could find are:\n",
    "- `maxIter` = 20\n",
    "- `rank` = 5\n",
    "- `regParam` = 0.1 (default)\n",
    "- `alpha` = 1000\n",
    "\n",
    "Let's _-validate this candidate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Let's cross-validate because we didn't actually do it in the grid search. We want to make sure that the selected model is not overfitting.\n",
    "\n",
    "The built-in cross validator in `Spark` keeps breaking when I try to use it, so let's build our own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = cr.get_spark_k_folds(train, k=k, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance for cv with our chosen parametrs\n",
    "als_cv = ALS(maxIter=best_max_iter,\n",
    "          rank=best_rank,\n",
    "          userCol='account_id',\n",
    "          itemCol='comic_id',\n",
    "          ratingCol='bought',\n",
    "          implicitPrefs=True,\n",
    "          regParam=best_reg,\n",
    "          alpha=best_alpha,\n",
    "          coldStartStrategy='drop', # we want to drop so can get through CV\n",
    "          seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = cr.get_cv_errors(folds, als_cv, eval_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that # of errors = k\n",
    "k == len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(errors), np.std(errors) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks stable. Let's go with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Candidate Model\n",
    "\n",
    "Test vs our holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_iter = 20\n",
    "best_reg = 0.1\n",
    "best_rank = 5\n",
    "best_alpha = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance and fit model\n",
    "als = ALS(maxIter=best_max_iter,\n",
    "          rank=best_rank,\n",
    "          userCol='account_id',\n",
    "          itemCol='comic_id',\n",
    "          ratingCol='bought',\n",
    "          implicitPrefs=True,\n",
    "          regParam=best_reg,\n",
    "          alpha=best_alpha,\n",
    "          coldStartStrategy='drop', # To get our eval\n",
    "          seed=random_seed)\n",
    "model_use = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test\n",
    "test_preds = model_use.transform(test)\n",
    "\n",
    "# Evaluate test\n",
    "test_rmse = eval_reg.evaluate(test_preds)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is unexpected. Test error being noticeably lower than train error usually indicates an unknown fit. Since we trained on 'train' data we would expect test error to be at minimum as worse AND _probably_ a little worse than train. Not less than.\n",
    "\n",
    "It's not THAT much better, but need to make note of it. For now we need to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance and fit model\n",
    "als = ALS(maxIter=best_max_iter,\n",
    "          rank=best_rank,\n",
    "          userCol='account_id',\n",
    "          itemCol='comic_id',\n",
    "          ratingCol='bought',\n",
    "          implicitPrefs=True,\n",
    "          regParam=best_reg,\n",
    "          alpha=best_alpha,\n",
    "          coldStartStrategy='nan', # To get our eval\n",
    "          seed=random_seed)\n",
    "model_use = als.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the item factors for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = model_use.itemFactors.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.to_pickle(\"support_data/item_factors_20190916.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_items = pd.read_pickle('support_data/item_factors_20190916.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top N recommendations for Single User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a reference list of `account_id`'s, for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_test = 2\n",
    "\n",
    "users = (sold.select(als.getUserCol())\n",
    "                          .sample(False\n",
    "                                  ,n_to_test/sold.count()\n",
    "                                  )\n",
    "        )\n",
    "users.persist()\n",
    "users.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed and wrote the functionality out to a function in `comic_recs.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing function!\n",
    "\n",
    "- Pass the function to a pandas dataframe. \n",
    "- Function will ask for an account_id.\n",
    "- Will return top n, n defined in parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=model_use, topn=5)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=model_use, topn=5)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=model_use, topn=10)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Seems realistic? Only three tests, but the results seem 'individualized' in the sense that there is no overlap between the sets (albeit small samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_use.save('models/als_use_20190916')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_rec_model = ALSModel.load('models/als_use_20190916')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=comic_rec_model, topn=10)\n",
    "top_n_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
