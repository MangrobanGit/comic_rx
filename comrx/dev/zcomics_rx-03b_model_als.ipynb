{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comics Rx\n",
    "## [A comic book recommendation system](https://github.com/MangrobanGit/comics_rx)\n",
    "<img src=\"https://images.unsplash.com/photo-1514329926535-7f6dbfbfb114?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2850&q=80\" width=\"400\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced Data: Grid Search + Cross-Validation\n",
    "\n",
    "This time, as explored in the EDA NB, let's consider removing customers who we feel have too few or too many purchases to influence the model in the intended way.\n",
    "\n",
    "Examples:\n",
    "- Too few - Customers who have only bought 1 comic (series).\n",
    "- Too many - Customers with > 1000 series (for example, think all eBay customers are rolled into one account number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "# %autoreload 1 #would be where you need to specify the files\n",
    "# %aimport comic_recs\n",
    "\n",
    "import pandas as pd # dataframes\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data storage\n",
    "from sqlalchemy import create_engine # SQL helper\n",
    "#import psycopg2 as psql #PostgreSQL DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.sql.types import (StructType, StructField, IntegerType\n",
    "#                                ,FloatType, LongType, StringType)\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, lit, isnan, when, count\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import (CrossValidator, ParamGridBuilder, \n",
    "                               TrainValidationSplit)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom\n",
    "import data_fcns as dfc\n",
    "import keys  # Custom keys lib\n",
    "import comic_recs as cr\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (conf.setMaster('local[*]')\n",
    "#         .set('spark.executor.memory', '1G') #https://stackoverflow.com/questions/48523629/spark-pyspark-an-error-occurred-while-trying-to-connect-to-the-java-server-127\n",
    "        .set('spark.driver.memory', '4G')\n",
    "        .set('spark.driver.maxResultSize', '1G'))\n",
    "#         .set('spark.executor.memory', '1G')\n",
    "#         .set('spark.driver.memory', '10G')\n",
    "#         .set('spark.driver.maxResultSize', '5G'))\n",
    "\n",
    "sc = pyspark.SparkContext().getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('./checkpoints')\n",
    "\n",
    "# spark.sparkContext.setCheckpointDir(\"hdfs://datalake/check_point_directory/als\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spark config\n",
    "# spark = pyspark.sql.SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"comic recs\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "#     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"8g\") \\\n",
    "#     .config(\"spark.master\", \"local[*]\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate SparkSession object\n",
    "# spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# # spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spark config\n",
    "# spark = pyspark.sql.SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"movie recommendation\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "#     .config(\"spark.driver.memory\", \"1g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"20g\") \\\n",
    "#     .config(\"spark.master\", \"local[*]\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "We've previously set aside the dataset into a `json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have previously created a version of the transactions table \n",
    "# and filtered it down.\n",
    "# sold = spark.read.json('raw_data/als_input_filtered.json')\n",
    "sold = sql_context.read.json('raw_data/als_input_filtered.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[account_id: bigint, bought: bigint, comic_id: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist the data\n",
    "sold.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61871"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sold.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Model\n",
    "\n",
    "Let's start with  train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "(train, test) = sold.randomSplit([.75, .25], seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure shapes make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46417 3\n"
     ]
    }
   ],
   "source": [
    "print(train.count(), len(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15454 3\n"
     ]
    }
   ],
   "source": [
    "print(test.count(), len(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "eval_reg = RegressionEvaluator(metricName=\"rmse\"\n",
    "                               , labelCol=\"bought\"\n",
    "                               , predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "# num_iterations = [10, 20, 25, 30]\n",
    "num_iterations = [20,25,30,35]\n",
    "ranks = [5]\n",
    "# ranks = [5, 10]\n",
    "# reg_params = [0.01, 0.1]\n",
    "reg_params = [0.1]\n",
    "alphas = [1000]\n",
    "# alphas = [40, 500, 1000, 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further subset into test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "(gs_train, gs_val) = train.randomSplit([(1-(1/3)), (1/3)], seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30854 3\n"
     ]
    }
   ],
   "source": [
    "print(gs_train.count(), len(gs_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15563 3\n"
     ]
    }
   ],
   "source": [
    "print(gs_val.count(), len(gs_val.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_iter = 25\n",
    "# rank = 5\n",
    "# reg = 0.1\n",
    "# alpha = 1000\n",
    "\n",
    "# als = ALS(maxIter=num_iter,\n",
    "#               rank=rank,\n",
    "#               userCol='account_id',\n",
    "#               itemCol='comic_id',\n",
    "#               ratingCol='bought',\n",
    "#               implicitPrefs=True,\n",
    "#               regParam=reg,\n",
    "#               alpha=alpha,\n",
    "#               coldStartStrategy='drop',  # Just for CV\n",
    "#               seed=41916)\n",
    "\n",
    "# model = als.fit(gs_train)\n",
    "\n",
    "# # Generate predictions on Test\n",
    "# predictions = model.transform(gs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 iterations, 5 latent factors, regularization=0.1, and alpha @ 1000 : validation error is 0.3612\n",
      "25 iterations, 5 latent factors, regularization=0.1, and alpha @ 1000 : validation error is 0.3557\n",
      "30 iterations, 5 latent factors, regularization=0.1, and alpha @ 1000 : validation error is 0.3509\n",
      "35 iterations, 5 latent factors, regularization=0.1, and alpha @ 1000 : validation error is 0.3469\n",
      "Total Runtime: 69.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model, params_errs = cr.train_ALS(gs_train, gs_val, eval_reg, \n",
    "                                        num_iterations, reg_params, \n",
    "                                        ranks, alphas)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the descriptive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_errs_rd_1 = params_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('support_data/params_errs_rd1_24seed.pkl', 'wb') as f:\n",
    "    pickle.dump(param_errs_rd_1, f)\n",
    "    \n",
    "# Example - load pickle\n",
    "# pickle_in = open(\"support_data/params_errs_rd1.pkl\",\"rb\")\n",
    "# pe1 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls support_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this to reload the Grid Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('support_data/params_errs_rd1_24seed.pkl', 'rb')\n",
    "params_errs = pickle.load(pickle_in)\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. Let's put `params_errs` into a dataframe and find the model with the lowest error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cols = ['max_iters', 'reg', 'rank', 'alpha', 'rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(params_errs, columns=gs_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_err = gs_df.rmse.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = gs_df.loc[gs_df['rmse']==min_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_iter = min_df['max_iters'].iloc[0]\n",
    "best_reg = min_df['reg'].iloc[0]\n",
    "best_rank = min_df['rank'].iloc[0]\n",
    "best_alpha = min_df['alpha'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some visual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rank_match = (gs_df['rank']==best_rank)\n",
    "gs_reg_match = (gs_df['reg']==best_reg)\n",
    "gs_iter_match = (gs_df['max_iters']==best_max_iter)\n",
    "gs_alpha_match = (gs_df['alpha']==best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank = gs_df.loc[(gs_reg_match & gs_iter_match & gs_alpha_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha = gs_df.loc[(gs_reg_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg = gs_df.loc[(gs_alpha_match & gs_iter_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter = gs_df.loc[(gs_alpha_match & gs_reg_match & gs_rank_match),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_vary_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So quick inspection on these, lets:\n",
    "- keep `rank` = 5\n",
    "- When compared to all the other combos, the differences in `alpha`s seem to not really move the needle > 500. so let's just call it `1000`\n",
    "- Keep `maxIter` at `20`; experience to date with my assets seems to show 20 is max capability before technical difficulties arise.\n",
    "- Similar with `alpha`, the marginal change in error due to changing `reg` is really small. So let's just assume the default `.01`.\n",
    "\n",
    "So, that means we are done selecting! We may really be pushing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing, let chart change in RMSE over change in alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df = gs_vary_alpha.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df['params_desc'] = (\n",
    "                                '\\u03B1=' + alpha_graph_df['alpha'].map(str) \n",
    "                                )\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot RMSE\n",
    "sns.set_color_codes(\"pastel\")\n",
    "\n",
    "values = alpha_graph_df['params_desc'].tolist()\n",
    "\n",
    "clrs = ['salmon' if (y == '\\u03B1=1000') else 'steelblue' for y in values ]\n",
    "\n",
    "s = sns.barplot(x=\"rmse\", y=\"params_desc\", data=alpha_graph_df,\n",
    "                label=\"RMSE\",\n",
    "                palette=clrs)\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"\",\n",
    "       xlabel=\"Max Iterations: 20 | Latent Factors: 5\")\n",
    "ax.set_title(\"Change in Error over Alpha\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "fig = s.get_figure()\n",
    "fig.savefig('support_data/alphas.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK**. Let's call it good. \n",
    "\n",
    "## Results \n",
    "Looks like the best parameters we could find are:\n",
    "- `maxIter` = 20\n",
    "- `rank` = 5\n",
    "- `regParam` = 0.1 (default)\n",
    "- `alpha` = 1000\n",
    "\n",
    "Let's cross-validate this candidate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Let's cross-validate because we didn't actually do it in the grid search. We want to make sure that the selected model is not overfitting.\n",
    "\n",
    "The built-in cross validator in `Spark` keeps breaking when I try to use it, so let's build our own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = cr.get_spark_k_folds(train, k=k, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance for cv with our chosen parametrs\n",
    "als_cv = ALS(maxIter=best_max_iter,\n",
    "          rank=best_rank,\n",
    "          userCol='account_id',\n",
    "          itemCol='comic_id',\n",
    "          ratingCol='bought',\n",
    "          implicitPrefs=True,\n",
    "          regParam=best_reg,\n",
    "          alpha=best_alpha,\n",
    "          coldStartStrategy='drop', # we want to drop so can get through CV\n",
    "          seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = cr.get_cv_errors(folds, als_cv, eval_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that # of errors = k\n",
    "k == len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(errors), np.std(errors) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks stable. Let's go with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Candidate Model\n",
    "\n",
    "Test vs our holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_iter = 20\n",
    "best_reg = 0.1\n",
    "best_rank = 5\n",
    "best_alpha = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance and fit model\n",
    "als = ALS(maxIter=best_max_iter,\n",
    "          rank=best_rank,\n",
    "          userCol='account_id',\n",
    "          itemCol='comic_id',\n",
    "          ratingCol='bought',\n",
    "          implicitPrefs=True,\n",
    "          regParam=best_reg,\n",
    "          alpha=best_alpha,\n",
    "          coldStartStrategy='drop', # To get our eval\n",
    "          seed=random_seed)\n",
    "model_use = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on test\n",
    "test_preds = model_use.transform(test)\n",
    "\n",
    "# Evaluate test\n",
    "test_rmse = eval_reg.evaluate(test_preds)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is unexpected. Test error being noticeably lower than train error usually indicates an unknown fit. Since we trained on 'train' data we would expect test error to be at minimum as worse AND _probably_ a little worse than train. Not less than.\n",
    "\n",
    "It's not THAT much better, but need to make note of it. For now we need to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALS instance and fit model\n",
    "als = ALS(maxIter=best_max_iter,\n",
    "          rank=best_rank,\n",
    "          userCol='account_id',\n",
    "          itemCol='comic_id',\n",
    "          ratingCol='bought',\n",
    "          implicitPrefs=True,\n",
    "          regParam=best_reg,\n",
    "          alpha=best_alpha,\n",
    "          coldStartStrategy='nan', # To get our eval\n",
    "          seed=random_seed)\n",
    "model_use = als.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the item factors for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = model_use.itemFactors.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.to_pickle(\"support_data/item_factors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_items = pd.read_pickle('support_data/item_factors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top N recommendations for Single User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a reference list of `account_id`'s, for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_test = 2\n",
    "\n",
    "users = (sold.select(als.getUserCol())\n",
    "                          .sample(False\n",
    "                                  ,n_to_test/sold.count()\n",
    "                                  )\n",
    "        )\n",
    "users.persist()\n",
    "users.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed and wrote the functionality out to a function in `comic_recs.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing function!\n",
    "\n",
    "- Pass the function to a pandas dataframe. \n",
    "- Function will ask for an account_id.\n",
    "- Will return top n, n defined in parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=model_use, topn=5)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=model_use, topn=5)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=model_use, topn=10)\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Seems realistic? Only three tests, but the results seem 'individualized' in the sense that there is no overlap between the sets (albeit small samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_use.save('models/als_use')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_rec_model = ALSModel.load('models/als_use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_df = cr.get_top_n_new_recs(spark=spark, model=comic_rec_model, topn=10)\n",
    "top_n_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
